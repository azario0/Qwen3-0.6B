{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-27T18:05:24.182752Z",
     "iopub.status.busy": "2025-08-27T18:05:24.182497Z",
     "iopub.status.idle": "2025-08-27T18:05:59.406230Z",
     "shell.execute_reply": "2025-08-27T18:05:59.405403Z",
     "shell.execute_reply.started": "2025-08-27T18:05:24.182727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import threading\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T18:07:04.424812Z",
     "iopub.status.busy": "2025-08-27T18:07:04.424510Z",
     "iopub.status.idle": "2025-08-27T18:07:04.430477Z",
     "shell.execute_reply": "2025-08-27T18:07:04.429692Z",
     "shell.execute_reply.started": "2025-08-27T18:07:04.424787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def stream_chat(messages, tokenizer, model, max_new_tokens=32768, enable_thinking=True):\n",
    "    \"\"\"\n",
    "    Stream model responses token-by-token in real time.\n",
    "    \"\"\"\n",
    "    # Prepare inputs\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Streamer to capture tokens\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=False)\n",
    "\n",
    "    # Run generation in a background thread\n",
    "    generation_kwargs = dict(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        streamer=streamer\n",
    "    )\n",
    "    thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    # Yield tokens in real time\n",
    "    for new_text in streamer:\n",
    "        yield new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T18:17:37.810100Z",
     "iopub.status.busy": "2025-08-27T18:17:37.809852Z",
     "iopub.status.idle": "2025-08-27T18:17:37.887980Z",
     "shell.execute_reply": "2025-08-27T18:17:37.887044Z",
     "shell.execute_reply.started": "2025-08-27T18:17:37.810074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ USAGE ------------------ #\n",
    "prompt = \"Is 9.10 bigger than 9.9 ?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "print(\"Assistant: \", end=\"\", flush=True)\n",
    "response = ''\n",
    "for token in stream_chat(messages, tokenizer, model,enable_thinking=False):\n",
    "    print(token, end=\"\", flush=True)  # real-time print\n",
    "    response += token\n",
    "print(\"\\n\\nFinal Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T18:07:06.495218Z",
     "iopub.status.busy": "2025-08-27T18:07:06.494487Z",
     "iopub.status.idle": "2025-08-27T18:07:42.316639Z",
     "shell.execute_reply": "2025-08-27T18:07:42.315917Z",
     "shell.execute_reply.started": "2025-08-27T18:07:06.495180Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ USAGE ------------------ #\n",
    "prompt = \"Is 9.10 bigger than 9.9 ?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "print(\"Assistant: \", end=\"\", flush=True)\n",
    "response = ''\n",
    "for token in stream_chat(messages, tokenizer, model):\n",
    "    print(token, end=\"\", flush=True)  # real-time print\n",
    "    response += token\n",
    "print(\"\\n\\nFinal Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T18:09:03.832698Z",
     "iopub.status.busy": "2025-08-27T18:09:03.832355Z",
     "iopub.status.idle": "2025-08-27T18:10:03.712914Z",
     "shell.execute_reply": "2025-08-27T18:10:03.712268Z",
     "shell.execute_reply.started": "2025-08-27T18:09:03.832643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ USAGE ------------------ #\n",
    "prompt = \"How many countries are there in Africa , please give me their names ?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "print(\"Assistant: \", end=\"\", flush=True)\n",
    "response = ''\n",
    "for token in stream_chat(messages, tokenizer, model):\n",
    "    print(token, end=\"\", flush=True)  # real-time print\n",
    "    response += token\n",
    "print(\"\\n\\nFinal Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
